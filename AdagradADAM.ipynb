{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9051ef94",
   "metadata": {},
   "source": [
    "# Predicting Hand-written Digits on the MNIST Dataset\n",
    "\n",
    "The MNIST dataset is a well-known, massive dataset of images of handwritten digits, 0 - 9. Thanks to its size, it's a common dataset in use for classification tasks. Our goal is to construct a classifier that, given an image of a handwritten digit, it predicts which digit it is.\n",
    "\n",
    "Here, we'll be trying to learn a good classifier using two different algorithms, Adagrad and ADAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50bb836",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset, loss and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4fdec19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Loading the data set\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "# For debugging purposes, you might want to restrict the number of labels\n",
    "# you are learning (e.g., you could begin with k=2).\n",
    "# However, in the end we want to have a classifier for all 10 digits.\n",
    "num_classes = 10\n",
    "\n",
    "# Filtering the data set to include only k labels\n",
    "train_X = train_X[train_y < num_classes]\n",
    "train_y = train_y[train_y < num_classes]\n",
    "test_X = test_X[test_y < num_classes]\n",
    "test_y = test_y[test_y < num_classes]\n",
    "\n",
    "# Collapsing the 28 x 28 feature matrix into a length 784 feature vector\n",
    "train_X = train_X.reshape(train_X.shape[0], train_X.shape[1] * train_X.shape[2]).astype(float)\n",
    "test_X = test_X.reshape(test_X.shape[0], test_X.shape[1] * test_X.shape[2]).astype(float)\n",
    "\n",
    "# Subtracting the mean and normalizing\n",
    "mean = np.mean(train_X, axis=0)\n",
    "print(mean.shape)\n",
    "train_X -= mean\n",
    "test_X -= mean\n",
    "std = np.std(train_X, axis=0)\n",
    "std = np.where(std == 0, 1, std)\n",
    "train_X /= std\n",
    "test_X /= std\n",
    "\n",
    "# Adding the all ones vector in place of a bias term\n",
    "train_X = np.hstack((train_X, np.ones((len(train_X), 1))))\n",
    "test_X = np.hstack((test_X, np.ones((len(test_X), 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccdd3c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_predict(X, W1, b1, W2, b2):\n",
    "    hidden_layer = np.maximum(0, np.dot(X, W1.T) + b1) # ReLU activation\n",
    "    scores = np.dot(hidden_layer, W2.T) + b2\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "    return y_pred\n",
    "\n",
    "def evaluate_accuracy(X, y, W1, b1, W2, b2):\n",
    "    y_pred = nn_predict(X, W1, b1, W2, b2)\n",
    "    accuracy = np.mean(y_pred == y)\n",
    "    return accuracy\n",
    "\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=1, keepdims=True)\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "def nn_loss_grad(X, y, W1, b1, W2, b2, reg):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients of the neural network.\n",
    "    Feature vectors have dimension n, there are K classes, and m training examples.\n",
    "\n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (K, n) containing weights.\n",
    "    - X: A numpy array of shape (m, n) containing the training feature vectors \n",
    "        (row i of X is the feature vector of the i-th training example).\n",
    "    - y: A numpy array of shape (m,) containing the training labels; y[i] = c means\n",
    "      that X[i] has label c, where 0 <= c < K.\n",
    "    - reg: (float) regularization strength (the gamma value from lecture)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W1; an array of same shape as W1\n",
    "    - gradient with respect to the offsets b1; an array of the same shape as b1\n",
    "    - gradient with respect to weights W2; an array of same shape as W2\n",
    "    - gradient with respect to the offsets b2; an array of the same shape as b2\n",
    "    \"\"\"\n",
    "\n",
    "    num_examples, _ = X.shape\n",
    "\n",
    "    hidden_layer = np.maximum(0, np.dot(X, W1.T) + b1) # ReLU activation\n",
    "    scores = np.dot(hidden_layer, W2.T) + b2\n",
    "    \n",
    "    # compute the class probabilities\n",
    "    probs = softmax(scores)\n",
    "\n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),y] + 1e-10)\n",
    "    data_loss = np.sum(corect_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W1*W1) + 0.5*reg*np.sum(W2*W2)\n",
    "    loss = data_loss + reg_loss\n",
    "\n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "\n",
    "    # backpropate the gradient to the parameters\n",
    "    # first backprop into parameters W2 and b2\n",
    "    dW2 = np.dot(hidden_layer.T, dscores).T\n",
    "    db2 = np.sum(dscores, axis=0)\n",
    "    # next backprop into hidden layer\n",
    "    dhidden = np.dot(dscores, W2)\n",
    "    # backprop the ReLU non-linearity\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "    # finally into W1,b1\n",
    "    dW1 = np.dot(X.T, dhidden).T\n",
    "    db1 = np.sum(dhidden, axis=0)\n",
    "\n",
    "    # add regularization gradient contribution\n",
    "    dW2 += reg * W2\n",
    "    dW1 += reg * W1\n",
    "\n",
    "    return loss, dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda47e4",
   "metadata": {},
   "source": [
    "# Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd879b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nn_train_adagrad(X, y, hidden_layer_size=100, reg=1e-3, num_iter=100, verbose=False):\n",
    "     \"\"\"\n",
    "     Train an neural network with a single hidden layer using  adagrad.\n",
    "\n",
    "     Inputs:\n",
    "     - X: A numpy array of shape (m, n) containing the training feature vectors \n",
    "          (row i of X is the feature vector of the i-th training example).\n",
    "     - y: A numpy array of shape (m,) containing the training labels; y[i] = c means\n",
    "          that X[i] has label c, where 0 <= c < K.\n",
    "     - hidden_layer_size: (int) size of the hidden layer.\n",
    "     - gamma: (float) regularization strength.\n",
    "     - num_iters: (integer) number of gradient descent iterations.\n",
    "     - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "     Outputs:\n",
    "     - W1 : matrix of weights of the first layer\n",
    "     - b1 : vector of offsets of the first layer\n",
    "     - W1 : matrix of weights of the output layer\n",
    "     - b1 : vector of offsets of the output layer\n",
    "     \"\"\"\n",
    "\n",
    "     _, num_features = X.shape\n",
    "\n",
    "     # initialize parameters randomly\n",
    "     W1 = 0.01 * np.random.randn(hidden_layer_size, num_features)\n",
    "     b1 = np.zeros(hidden_layer_size)\n",
    "     W2 = 0.01 * np.random.randn(num_classes, hidden_layer_size)\n",
    "     b2 = np.zeros(num_classes)\n",
    "\n",
    "     #storage\n",
    "     cache_W1 = np.zeros_like(W1)\n",
    "     cache_b1 = np.zeros_like(b1)\n",
    "     cache_W2 = np.zeros_like(W2)\n",
    "     cache_b2 = np.zeros_like(b2)\n",
    "\n",
    "     for i in range(num_iter):   \n",
    "          loss, dW1, db1, dW2, db2 = nn_loss_grad(X, y, W1, b1, W2, b2, reg)\n",
    "          cache_W1 += dW1 ** 2\n",
    "          cache_b1 += db1 ** 2\n",
    "          cache_W2 += dW2 ** 2\n",
    "          cache_b2 += db2 ** 2\n",
    "          W1 -= dW1 / (np.sqrt(cache_W1 + 1e-10) )\n",
    "          b1 -= db1 / (np.sqrt(cache_b1 + 1e-10) )\n",
    "          W2 -= dW2 / (np.sqrt(cache_W2 + 1e-10) )\n",
    "          b2 -= db2 / (np.sqrt(cache_b2 + 1e-10) )\n",
    "          if verbose and i % 10 == 0:\n",
    "               print(\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "     return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 2.304814\n",
      "iteration 10: loss 35.793722\n",
      "iteration 20: loss 26.589574\n",
      "iteration 30: loss 21.391386\n",
      "iteration 40: loss 17.826571\n",
      "iteration 50: loss 15.211064\n",
      "iteration 60: loss 13.012247\n",
      "iteration 70: loss 11.420146\n",
      "iteration 80: loss 10.036550\n",
      "iteration 90: loss 8.916381\n",
      "Train accuracy:  0.9753333333333334\n",
      "Test accuracy:  0.9475\n"
     ]
    }
   ],
   "source": [
    "#predict using Adagrad\n",
    "W1, b1, W2, b2 = nn_train_adagrad(train_X, train_y, hidden_layer_size=100, reg=1e-3, num_iter=100, verbose=True)\n",
    "train_accuracy = evaluate_accuracy(train_X, train_y, W1, b1, W2, b2)\n",
    "test_accuracy = evaluate_accuracy(test_X, test_y, W1, b1, W2, b2)\n",
    "print(\"Train accuracy: \", train_accuracy)\n",
    "print(\"Test accuracy: \", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4042b2",
   "metadata": {},
   "source": [
    "# ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94e9dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train_adam(X, y, hidden_layer_size=100, reg=1e-3, num_iter=100, verbose=False):\n",
    "     \"\"\"\n",
    "     Train an neural network with a single hidden layer using  ADAM.\n",
    "\n",
    "     Inputs:\n",
    "     - X: A numpy array of shape (m, n) containing the training feature vectors \n",
    "          (row i of X is the feature vector of the i-th training example).\n",
    "     - y: A numpy array of shape (m,) containing the training labels; y[i] = c means\n",
    "          that X[i] has label c, where 0 <= c < K.\n",
    "     - hidden_layer_size: (int) size of the hidden layer.\n",
    "     - gamma: (float) regularization strength.\n",
    "     - num_iters: (integer) number of gradient descent iterations.\n",
    "     - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "     Outputs:\n",
    "     - W1 : matrix of weights of the first layer\n",
    "     - b1 : vector of offsets of the first layer\n",
    "     - W1 : matrix of weights of the output layer\n",
    "     - b1 : vector of offsets of the output layer\n",
    "     \"\"\"\n",
    "\n",
    "     _, num_features = X.shape\n",
    "\n",
    "     # initialize parameters randomly\n",
    "     W1 = 0.01 * np.random.randn(hidden_layer_size, num_features)\n",
    "     b1 = np.zeros(hidden_layer_size)\n",
    "     W2 = 0.01 * np.random.randn(num_classes, hidden_layer_size)\n",
    "     b2 = np.zeros(num_classes)\n",
    "     beta1 = 0.9\n",
    "     beta2 = 0.999\n",
    "\n",
    "     #storage\n",
    "     m_W1 = np.zeros_like(W1)\n",
    "     m_b1 = np.zeros_like(b1)\n",
    "     m_W2 = np.zeros_like(W2)\n",
    "     m_b2 = np.zeros_like(b2)\n",
    "     v_W1 = np.zeros_like(W1)\n",
    "     v_b1 = np.zeros_like(b1)\n",
    "     v_W2 = np.zeros_like(W2)\n",
    "     v_b2 = np.zeros_like(b2)\n",
    "     \n",
    "     for i in range(num_iter):\n",
    "          loss, dW1, db1, dW2, db2 = nn_loss_grad(X, y, W1, b1, W2, b2, reg)\n",
    "          m_W1 = beta1 * m_W1 + (1 - beta1) * dW1\n",
    "          m_b1 = beta1 * m_b1 + (1 - beta1) * db1\n",
    "          m_W2 = beta1 * m_W2 + (1 - beta1) * dW2\n",
    "          m_b2 = beta1 * m_b2 + (1 - beta1) * db2\n",
    "          v_W1 = beta2 * v_W1 + (1 - beta2) * dW1 ** 2\n",
    "          v_b1 = beta2 * v_b1 + (1 - beta2) * db1 ** 2\n",
    "          v_W2 = beta2 * v_W2 + (1 - beta2) * dW2 ** 2\n",
    "          v_b2 = beta2 * v_b2 + (1 - beta2) * db2 ** 2\n",
    "          W1 = W1 - m_W1 / (np.sqrt(v_W1) + 1e-10)\n",
    "          b1 = b1 - m_b1 / (np.sqrt(v_b1) + 1e-10)\n",
    "          W2 = W2 - m_W2 / (np.sqrt(v_W2) + 1e-10)\n",
    "          b2 = b2 - m_b2 / (np.sqrt(v_b2) + 1e-10)\n",
    "          if verbose and i % 10 == 0:\n",
    "               print(\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "     return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 2.305880\n",
      "iteration 10: loss 4445.414071\n",
      "iteration 20: loss 6531.377248\n",
      "iteration 30: loss 4932.266780\n",
      "iteration 40: loss 2938.705231\n",
      "iteration 50: loss 1639.890035\n",
      "iteration 60: loss 935.877995\n",
      "iteration 70: loss 577.587407\n",
      "iteration 80: loss 386.925530\n",
      "iteration 90: loss 287.989865\n",
      "Train accuracy:  0.9730166666666666\n",
      "Test accuracy:  0.9385\n"
     ]
    }
   ],
   "source": [
    "#predict using Adam\n",
    "W1, b1, W2, b2 = nn_train_adam(train_X, train_y, hidden_layer_size=100, reg=1e-3, num_iter=100, verbose=True)\n",
    "train_accuracy = evaluate_accuracy(train_X, train_y, W1, b1, W2, b2)\n",
    "test_accuracy = evaluate_accuracy(test_X, test_y, W1, b1, W2, b2)\n",
    "print(\"Train accuracy: \", train_accuracy)\n",
    "print(\"Test accuracy: \", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ec450",
   "metadata": {},
   "source": [
    "# ADAM with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a88696a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train_adam_stoch(X, y, hidden_layer_size=100, reg=1e-3, num_stoch=10, num_iter=1000000, verbose=False):\n",
    "     \"\"\"\n",
    "     Train an neural network with a single hidden layer using  ADAM and stochastic evaluations\n",
    "     of the gradient.\n",
    "\n",
    "     Inputs:\n",
    "     - X: A numpy array of shape (m, n) containing the training feature vectors \n",
    "          (row i of X is the feature vector of the i-th training example).\n",
    "     - y: A numpy array of shape (m,) containing the training labels; y[i] = c means\n",
    "          that X[i] has label c, where 0 <= c < K.\n",
    "     - hidden_layer_size: (int) size of the hidden layer.\n",
    "     - reg: (float) regularization strength.\n",
    "     - num_stoch: (integer) number of training examples used for evaluating the\n",
    "          stochastic gradient\n",
    "     - num_iters: (integer) number of gradient descent iterations.\n",
    "     - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "     Outputs:\n",
    "     - W1 : matrix of weights of the first layer\n",
    "     - b1 : vector of offsets of the first layer\n",
    "     - W1 : matrix of weights of the output layer\n",
    "     - b1 : vector of offsets of the output layer\n",
    "     \"\"\"\n",
    "\n",
    "     _, num_features = X.shape\n",
    "\n",
    "     # initialize parameters randomly\n",
    "     W1 = 0.01 * np.random.randn(hidden_layer_size, num_features)\n",
    "     b1 = np.zeros(hidden_layer_size)\n",
    "     W2 = 0.01 * np.random.randn(num_classes, hidden_layer_size)\n",
    "     b2 = np.zeros(num_classes)\n",
    "     beta1 = 0.9\n",
    "     beta2 = 0.999\n",
    "\n",
    "     #storage\n",
    "     m_W1 = np.zeros_like(W1)\n",
    "     m_b1 = np.zeros_like(b1)\n",
    "     m_W2 = np.zeros_like(W2)\n",
    "     m_b2 = np.zeros_like(b2)\n",
    "     v_W1 = np.zeros_like(W1)\n",
    "     v_b1 = np.zeros_like(b1)\n",
    "     v_W2 = np.zeros_like(W2)\n",
    "     v_b2 = np.zeros_like(b2)\n",
    "     \n",
    "     for i in range(num_iter):\n",
    "          index = np.random.choice(X.shape[0], num_stoch, replace=False)\n",
    "          X_stoch = X[index]\n",
    "          y_stoch = y[index]\n",
    "          loss, dW1, db1, dW2, db2 = nn_loss_grad(X_stoch, y_stoch, W1, b1, W2, b2, reg)\n",
    "          m_W1 = beta1 * m_W1 + (1 - beta1) * dW1\n",
    "          m_b1 = beta1 * m_b1 + (1 - beta1) * db1\n",
    "          m_W2 = beta1 * m_W2 + (1 - beta1) * dW2\n",
    "          m_b2 = beta1 * m_b2 + (1 - beta1) * db2\n",
    "          v_W1 = beta2 * v_W1 + (1 - beta2) * dW1 ** 2\n",
    "          v_b1 = beta2 * v_b1 + (1 - beta2) * db1 ** 2\n",
    "          v_W2 = beta2 * v_W2 + (1 - beta2) * dW2 ** 2\n",
    "          v_b2 = beta2 * v_b2 + (1 - beta2) * db2 ** 2\n",
    "          W1 = W1 - m_W1 / (np.sqrt(v_W1) + 1e-10)\n",
    "          b1 = b1 - m_b1 / (np.sqrt(v_b1) + 1e-10)\n",
    "          W2 = W2 - m_W2 / (np.sqrt(v_W2) + 1e-10)\n",
    "          b2 = b2 - m_b2 / (np.sqrt(v_b2) + 1e-10)\n",
    "          if verbose and i % 100 == 0:\n",
    "               print(\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "     return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 2.317678\n",
      "iteration 100: loss 174332.270786\n",
      "iteration 200: loss 158407.492954\n",
      "iteration 300: loss 148134.371158\n",
      "iteration 400: loss 138888.443435\n",
      "iteration 500: loss 127680.454754\n",
      "iteration 600: loss 117720.739075\n",
      "iteration 700: loss 108337.389703\n",
      "iteration 800: loss 102850.099982\n",
      "iteration 900: loss 96426.775901\n",
      "iteration 1000: loss 90835.701148\n",
      "iteration 1100: loss 85982.654150\n",
      "iteration 1200: loss 80316.471789\n",
      "iteration 1300: loss 75041.738042\n",
      "iteration 1400: loss 69452.834070\n",
      "iteration 1500: loss 65783.347702\n",
      "iteration 1600: loss 62116.760334\n",
      "iteration 1700: loss 59293.642066\n",
      "iteration 1800: loss 55588.731878\n",
      "iteration 1900: loss 52156.664698\n",
      "iteration 2000: loss 50152.572781\n",
      "iteration 2100: loss 49009.163089\n",
      "iteration 2200: loss 45989.530143\n",
      "iteration 2300: loss 46762.389024\n",
      "iteration 2400: loss 44776.709190\n",
      "iteration 2500: loss 42499.679972\n",
      "iteration 2600: loss 40513.380763\n",
      "iteration 2700: loss 38185.285446\n",
      "iteration 2800: loss 35981.930684\n",
      "iteration 2900: loss 34755.521144\n",
      "iteration 3000: loss 33260.207533\n",
      "iteration 3100: loss 32740.434912\n",
      "iteration 3200: loss 31675.141838\n",
      "iteration 3300: loss 30335.353746\n",
      "iteration 3400: loss 28683.931305\n",
      "iteration 3500: loss 26804.518398\n",
      "iteration 3600: loss 26337.970425\n",
      "iteration 3700: loss 25951.980177\n",
      "iteration 3800: loss 25076.083066\n",
      "iteration 3900: loss 24687.312185\n",
      "iteration 4000: loss 24690.182100\n",
      "iteration 4100: loss 24950.318168\n",
      "iteration 4200: loss 24758.633201\n",
      "iteration 4300: loss 24455.163421\n",
      "iteration 4400: loss 23843.448963\n",
      "iteration 4500: loss 23159.343326\n",
      "iteration 4600: loss 22460.618977\n",
      "iteration 4700: loss 21491.454905\n",
      "iteration 4800: loss 21064.119436\n",
      "iteration 4900: loss 20548.001133\n",
      "iteration 5000: loss 19800.980890\n",
      "iteration 5100: loss 18817.131980\n",
      "iteration 5200: loss 18461.902246\n",
      "iteration 5300: loss 18201.515606\n",
      "iteration 5400: loss 19180.465909\n",
      "iteration 5500: loss 19745.548235\n",
      "iteration 5600: loss 19354.955044\n",
      "iteration 5700: loss 18743.622610\n",
      "iteration 5800: loss 20024.265592\n",
      "iteration 5900: loss 22053.570905\n",
      "iteration 6000: loss 22787.989161\n",
      "iteration 6100: loss 22355.545162\n",
      "iteration 6200: loss 22063.615791\n",
      "iteration 6300: loss 20949.688580\n",
      "iteration 6400: loss 20584.473964\n",
      "iteration 6500: loss 20280.147061\n",
      "iteration 6600: loss 20053.417189\n",
      "iteration 6700: loss 18931.656172\n",
      "iteration 6800: loss 18821.606795\n",
      "iteration 6900: loss 18457.937021\n",
      "iteration 7000: loss 17390.198033\n",
      "iteration 7100: loss 16704.788096\n",
      "iteration 7200: loss 16583.317203\n",
      "iteration 7300: loss 16681.070443\n",
      "iteration 7400: loss 16694.924051\n",
      "iteration 7500: loss 16239.075547\n",
      "iteration 7600: loss 16228.832543\n",
      "iteration 7700: loss 15062.823425\n",
      "iteration 7800: loss 14803.298407\n",
      "iteration 7900: loss 14873.642405\n",
      "iteration 8000: loss 13944.226567\n",
      "iteration 8100: loss 13617.992915\n",
      "iteration 8200: loss 13614.940174\n",
      "iteration 8300: loss 13557.612455\n",
      "iteration 8400: loss 12759.954546\n",
      "iteration 8500: loss 12260.871274\n",
      "iteration 8600: loss 12086.216210\n",
      "iteration 8700: loss 12758.791605\n",
      "iteration 8800: loss 12148.103557\n",
      "iteration 8900: loss 12638.769460\n",
      "iteration 9000: loss 12724.507909\n",
      "iteration 9100: loss 12703.621885\n",
      "iteration 9200: loss 12497.392757\n",
      "iteration 9300: loss 12601.781183\n",
      "iteration 9400: loss 12571.525546\n",
      "iteration 9500: loss 12117.991313\n",
      "iteration 9600: loss 11766.431864\n",
      "iteration 9700: loss 11125.660976\n",
      "iteration 9800: loss 10764.458706\n",
      "iteration 9900: loss 10333.382269\n",
      "Train accuracy:  0.8777166666666667\n",
      "Test accuracy:  0.8788\n"
     ]
    }
   ],
   "source": [
    "#predict using Adam-SGD\n",
    "W1, b1, W2, b2 = nn_train_adam_stoch(train_X, train_y, hidden_layer_size=250, reg=1e-3, num_iter=10000, verbose=True)\n",
    "train_accuracy = evaluate_accuracy(train_X, train_y, W1, b1, W2, b2)\n",
    "test_accuracy = evaluate_accuracy(test_X, test_y, W1, b1, W2, b2)\n",
    "print(\"Train accuracy: \", train_accuracy)\n",
    "print(\"Test accuracy: \", test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
